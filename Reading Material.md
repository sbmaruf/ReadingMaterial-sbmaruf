Paper that I Read
---
1. Collobert et al, Natural Language Processing (Almost) from Scratch, [Link](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)
2. Lample et al, Neural Architectures for Named Entity Recognition. [Link](https://arxiv.org/abs/1603.01360)
3. Guillaume Lample et al, Word Translation Without Parallel Data. [Link](https://arxiv.org/abs/1710.04087)
4. Joty&Nguyen et al, A Neural Local Coherence Model. [Link](https://raihanjoty.github.io/papers/nguyen-joty-acl-17.pdf)
5. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. [Link](https://arxiv.org/abs/1711.04903)
6. Goodfellow et al, Generative Adversarial Networks. [Link](https://arxiv.org/abs/1406.2661) [Slide](https://drive.google.com/file/d/1qd9J7RDXh7q6qsncGbmkZ-Wj9W9bOWdh/view?usp=sharing)
7. Arjovsky et al, Wasserstein GAN. [Link](https://arxiv.org/abs/1701.07875)
8. Arjovsky et al, Towards Principled Methods for Training Generative Adversarial Networks. [Link](https://arxiv.org/abs/1701.04862) 
9. Salimans et al, Improved Techniques for Training GANs[Link](https://arxiv.org/abs/1606.03498)
10. Ganin et al, Domain-Adversarial Training of Neural Networks. [Link](https://arxiv.org/abs/1505.07818) [Slide](https://drive.google.com/file/d/1qd9J7RDXh7q6qsncGbmkZ-Wj9W9bOWdh/view?usp=sharing)
11. Zhu et al, (Cycle-GAN) Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. [Link](https://arxiv.org/abs/1703.10593)
12. Zhao et al, Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture. [Link](http://sleep.csail.mit.edu/files/rfsleep-paper.pdf) [Slide](https://drive.google.com/file/d/1qd9J7RDXh7q6qsncGbmkZ-Wj9W9bOWdh/view?usp=sharing)
13. Zhang et al, Aspect-augmented Adversarial Networks for Domain Adaptation. [Link](https://arxiv.org/abs/1701.00188) [Slide](https://drive.google.com/file/d/1qd9J7RDXh7q6qsncGbmkZ-Wj9W9bOWdh/view?usp=sharing)
14. Chen et al, Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification.[Link](https://arxiv.org/abs/1606.01614)
15. Miyato et al, Spectral Normalization for Generative Adversarial Networks. [Link](https://openreview.net/forum?id=B1QRgziT-)
16. Sutskever et al, Sequence to Sequence Learning with Neural Networks. [Link](https://arxiv.org/abs/1409.3215) [Slide](https://drive.google.com/file/d/1W2BaUNc5IqaDypNiXcb0MweOtCetUqZm/view)
17. Cho et al, Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation. [Link](https://www.aclweb.org/anthology/D14-1179) [Slide](https://drive.google.com/file/d/1RYUV3YmPrVoRTujaJ0kt6jyD6-4a8Zie/view)
18. Badanau et al, Neural Machine Translation by Jointly Learning to Align and Translate. [Link](Neural Machine Translation by Jointly Learning to Align and Translate) [Slide](https://drive.google.com/file/d/1niMR8LX77DnP_iPzjNRauOdz1wjd_eXp/view)
19. Luong et al, Effective Approaches to Attention-based Neural Machine Translation. [Link](https://arxiv.org/abs/1508.04025) [Slide](https://drive.google.com/file/d/1rzX97LRgtQdg6YmVeAq92oLqXGCEjhpb/view)




Tutorial:
---
1. Understanding LSTM Networks [Link](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
2. The Unreasonable Effectiveness of Recurrent Neural Networks. (http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
